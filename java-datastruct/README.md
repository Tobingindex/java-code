# 数据结构

## 算法

### 递归

递归是一种使用非常广泛的算法（或编程技巧）。只要同时满足以下三个条件，可以使用递归解决。

+ 一个问题的解可以分解成几个子问题的解（子问题就是数据规模更小的问题）
+ 问题与分解之后的子问题，除了数据规模不同，求解思路完全相同
+ 存在递归终止条件

写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。（**写出递推公式，找出终止条件**）

**只要遇到递归，就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。**

如果一个问题A可以分解为若干子问题B/C/D，可以假设子问题B/C/D已经解决，在此基础上思考如何解决问题A。在思考时，只需要思考问题A与子问题B/C/D两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。

编写递归代码时，需要警惕一下几种情况：

+ **警惕堆栈溢出**，JVM中系统栈的大小是有限的，如果递归调用深度太深，会导致堆栈溢出。这时可以通过限制递归调用深度来避免，超过一定深度，直接抛出异常
+ **警惕重复运算**，递归运算时，可以会存在重复计算的问题，为了避免重复计算，可以通过一个数据结构来保存已经求解过的f(k)。当递归调用到f(k)时，先看是否已经求解过。

在时间效率上，递归代码有很多函数调用，当这些函数调用的数量很大时，会积聚成一个可观的时间成本。

在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，因此在分析递归复杂度时，需要额外考虑这部分的开销。

总而言之，递归代码有利有弊。利是递归代码表达力强，写起来简洁；弊是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多函数调用会耗时较多等问题。

需要注意的是，递归本身是通过借助栈来实现，只不过使用的栈是系统或虚拟机本身提供的。我们也可以内存堆上实现栈，手动模拟入栈、出栈过程。

对于递归调试，几乎很难使用IDE的单步跟踪功能，这是可以**通过打印日志，结合条件断点**的方式来进行调试。

### 排序算法

常用的排序算法有：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序等。按照时间复杂度，可以将它们分成三类。

| 排序算法         | 时间复杂度 | 是否基于比较       |
| ---------------- | ---------- | ------------------ |
| 冒泡、插入、选择 | O(n*n)     | :heavy_check_mark: |
| 快排、归并       | O(n*logn)  | :heavy_check_mark: |
| 桶、计数、基数   | O(n)       | :x:                |

评价、分析一个算法可以通过以下几方面入手。

**排序算法的执行效率**

排序算法的执行效率可以从以下几方面来衡量：

+ **最好情况、最坏情况、平均情况时间复杂度**：对于排序的数据，有的接近有序，有的完全无序。有序度不同对排序的执行时间有影响，因此需要知道排序算法在不同数据下的性能表现。
+ **时间复杂度的系数、常数、低阶**：时间复杂度是反映数据规模很多时的增长趋势，因此表示时会忽略系数、常数、低阶。但实际开发中，排序可能是10个、100个等小规模数据，因此，对同一阶时间复杂度的排序算法性能对比时，要考虑系数、常数、低阶。
+ **比较次数和交换次数**：基于比较的排序算法执行过程中，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。因此分析执行效率时，要把比较次数和交换次数也考虑进去。

**排序算法的内存消耗**

算法的消耗的内存可以通过时间复杂度来衡量，排序算法亦是如此。对于排序算法，还引入了新概念，**原地排序**，用于特指时间复杂度为O(1)的排序算法。

**排序算法的稳定性**

排序算法中，还有一个重要的指标，稳定性。稳定性指的是，如果待排序的序列中存在值相等的元素，经过排序后，相同元素之间原有的先后顺序不变。

<font style="color:red">**为什么需要考虑稳定性？**</font>

> 在时间的项目开发中，需要排序的不仅仅是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。如订单系统中的“订单”有两个属性，一个是下单时间，另外一个是订单金额。如果需要按照金额大小进行排序，对于金额相同的订单，按照下单时间从早到晚排序，需要怎么实现呢？
>
> 最容易想到的是，先按照金额对订单进行排序，然后再遍历之后的订单数据，对于每个金额相同的小区间按照下单时间排序。这种排序思路简单，但是实现起来比较复杂。
>
> 但是如果使用稳定算法，实现起来就非常简洁。如先按照订单下单时间排序，排序完成之后，在通过稳定排序算法，按照订单金额重新排序。两遍排序之后，就可以的上述的效果。
>
> 除此之外，在平时我们使用Windows资源管理器中，我们可能会先按照文件名排序，再按照文件类型排序，这样一来，就可以得到按文件类型排序，相同文件类型的按照文件名排序。这样也是利用了排序算法的稳定性。

#### 冒泡排序

冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻元素进行比较，看是否满足大小关系。如果不满足就进行交换。一次冒泡至少让一个元素移动到它应该的位置，重复n次，就完成了n个数据的排序工作。

上述的冒泡排序过程还可以进行优化，即当某次冒泡操作已经没有数据交换时，说明到达完全有序，不用再继续执行后续的冒泡操作。

**原地算法**：冒泡过程只涉及相邻数据的交换操作，只需要常量级的临时空间，因此空间复杂度为O(1)。

**稳定算法**：在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证算法稳定，当相邻元素大小相等时，不做交换。

**最好最坏时间复杂度**：最好情况，要排序数据已经有序，只需要1次冒泡操作即可结束，因此最好时间复杂度为O(n)。最好情况，排序数据时完全无序，需要进行n次冒泡操作，因此最好时间复杂度为O(n*n)。

**平均时间复杂度**：平均时间复杂度即加权平均期望时间复杂度，如果使用概率论分析起来会比较复杂，可以通过“**有序度**”和“**逆序度**”进行分析。

+ **有序度是数组中有序关系的元素对个数。**对于一个倒排数组，如6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。  
+ **逆序度 = 满有序度 - 有序度**  

冒泡排序中包含两个操作原子，比较和交换。没交换一次，有序度加1。不管算法怎么改进，交换次数总是确定，**即为逆序度，即n*(n-1)/2-初始有序度。**

最坏情况下，初始状态的有序度是 0，所以要进行 n\*(n-1)/2 次交换。最好情况下，初始状态的有序度是 n\*(n-1)/2，就不需要进行交换。我们可以取个中间值 n\*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。  即平均时间复杂度为O(n*n)

#### 插入排序

对于一个有序的数组，如果需要保证插入一个新元素仍然有序，可以在插入时遍历数组，找到数据应该插入的位置插入即可。

可以看出上面是一个动态的过程，保证动态插入的数据之后，数组仍然有序。对于一组静态的数据，也可以借鉴上述方法，进行插入排序，那就是插入排序算法的核心实现。

实现将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，即数组的对一个元素。插入算法的核心思想是**取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并且保证已排序区间数据一直有序，重复这个过程**。知道未排序区间中元素为空，算法结束。

插入排序中包含两种操作：元素比较和元素的移动。当需要将一个数据a插入到已排序区间时，需要拿a和已排序区间的元素依次比较大小，找到合适位置。找到合适位置之后，需要将插入点之后的元素顺序往后移动一位，腾出位置给元素a插入。（如果对于链表，可以不用挪动元素）

对于不同的查找插入点方法（从头到尾、从尾到头），元素比较的次数是有区别的，但对于一个给定的初始序列，移动操作的次数总是固定的，等于逆序度。

**原地算法**：插入排序不需要额外的空间，因此空间复杂度是O(1)。

**稳定算法**：插入排序中，对于值相同的元素，可以选择将其后面出现的元素，插入到前面出现的元素后面，可以保证前后有序性不变，是一种稳定算法。

**最好最坏时间复杂度**：最好情况，元素有序，不需要搬动任何数据，如果是从头到尾在有序数据中查找插入位置，每次只需要比较一个数据就能确定插入的位置。因此最好时间复杂度为O(n)。最好情况，数组倒序，每次插入相当于在数组第一个位置插入新的数据，移动大量元素。因此最坏时间复杂度为O(n*n)。

**平均时间复杂度**：在数组中插入一个数据的平均时间复杂度是 O(n)，对于插入排序，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n*n)

#### 选择排序

选择排序思路类似于插入，也将数组分为排序区间和未排序区间。但须选择排序每次会从未排序区间找到最小元素，将其放到已排序的区间尾部。

**原地算法**：显然选择排序可爱内分组的为O(1)。

**非稳定算法**：可以注意，选择排序每次要找剩余元素中最小值，并和前面的元素交换位置。从最小元素位置到要交换位置元素之间，开年会存在于被交换元素相等的数据，因此交换操作会破坏稳定性。

**时间复杂度**：选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n )。

<font style="color:red">**为什么插入排序优于冒泡排序？**</font>

不论如何，冒泡排序的元素交换次数都是一个固定值，为原始数据的逆序度。插入排序也是，不论怎么优化，元素移动的次数等于元素数据的逆序度。

但从代码实现角度，冒泡排序的数据交换要比插入排序的数据移动要复杂。（冒泡排序需要3个赋值操作，插入操作需要1个）

> 补充：[插入排序的优化](https://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F)

| 排序算法 | 是否原地排序       | 是否稳定           | 最好   | 最坏   | 平均   |
| -------- | ------------------ | ------------------ | ------ | ------ | ------ |
| 冒泡排序 | :heavy_check_mark: | :heavy_check_mark: | O(n)   | O(n*n) | O(n*n) |
| 插入排序 | :heavy_check_mark: | :heavy_check_mark: | O(n)   | O(n*n) | O(n*n) |
| 选择排序 | :heavy_check_mark: | :x:                | O(n*n) | O(n*n) | O(n*n) |

在三种时间复杂度为O(n*n)的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面，而插入排序还是挺有用的。  

![base_sorte_exec_principal](https://gitee.com/tobing/imagebed/raw/master/base_sorte_exec_principal.png)

#### 归并排序

对于要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。  

> 分治思想和递归思想很像，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧。

**稳定算法**：归并算法稳定性关键看合并过程的merge()函数，在合并过程中对于相同值，可以优先把前面部分先放到临时数组，就可以保证合并前后顺序不变。

**时间复杂度**：归并排序涉及递归，时间复杂度的分析稍微有点复杂。可以发现递归的时间复杂度可以写成递推公式。归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是(nlogn)。  

**空间复杂度**：归并排序并不是原地排序，在每次递归的时候需要额外的数组来储存每次合并的结果。而这些临时的空间会在每次使用完毕会释放，因此并不需要累加，同时这些空间占用不会超过n个数据大小，因此空间复杂度为O(n)。

```c
// 归并排序算法, A 是数组，n 表示数组大小
merge_sort(A, n) {
	merge_sort_c(A, 0, n-1)；
}
// 递归调用函数
merge_sort_c(A, p, r) {
    // 递归终止条件
    if p >= r then return
        
    // 取 p 到 r 之间的中间位置 q
    q = (p+r) / 2
    // 分治递归
    merge_sort_c(A, p, q)
    merge_sort_c(A, q+1, r)
    // 将 A[p...q] 和 A[q+1...r] 合并为 A[p...r]
    merge(A[p...r], A[p...q], A[q+1...r])
}
```

#### 快速排序

同样地，快排也是基于分治算法实现。

对于要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将pivot 放到中间。

经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。  

```c
// 快速排序，A 是数组，n 表示数组的大小
quick_sort(A, n) {
	quick_sort_c(A, 0, n-1)
}
// 快速排序递归函数，p,r 为下标
quick_sort_c(A, p, r) {
	if p >= r then return
	
	q = partition(A, p, r) // 获取分区点
	quick_sort_c(A, p, q-1)
	quick_sort_c(A, q+1, r)
}
```

>partition()分区函数就是随机选中一个元素作为pivot的下标，然后对A[p...r]分区，函数返回pivot下标。
>
>如果不考虑空间消耗，partition()可以编写的很简单，申请两个临时数组X和Y，遍历遍历 A[p…r]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。  
>
>如果希望快排是原地算法，那么partition()分区函数就不能占用太多额外空间，需要在A[p…r]原地完成。这个过程有点类似选择排序。通过游标 i 把 A[p…r-1] 分成两部分。A[p…i-1] 的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i…r-1] 是“未处理区间”。我们每次都从未处理的区间 A[i…r-1] 中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i] 的位置。  

**原地算法**：由上面的分区函数可以会知道，快排可以是原地算法。

**不稳定算法**：从分区的过程可以看出，快排是不稳定的算法。

**时间复杂度**：每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn) 。

归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。  

<font style="color:red">**如何利用快排的思想以O(n)时间复杂度内求无序数组中的第K大元素？**</font>

+ 将数组区间A[0...n-1]的最后一个元素A[n-1]作为pivot，对数组A[0...n-1]原地分区，将数组分成三部分：A[0...p-1]、A[p]、A[p+1...n-1]。

+ 如果p+1=K，则A[p]就是要求的元素；
+ 如果K>p+1,说明第K大元素出现在A[p+1...n-1]区间，重复第一步在A[p+1...n-1]区间查找。
+ 同理，如果K<p+1，说明第K大元素出现在A[p+1...n-1]区间，重复第一步在A[0...p-1]区间查找。

在上述的过程中，第一次分区查找时，需要对大小为 n 的数组执行分区操作，需要遍历 n 个元素。第二次分区查找时，只需要对大小为 n/2 的数组执行分区操作，需要遍历 n/2 个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.……直到区间缩小为 1。  

## 线性结构

### 数组

数组（Array）是一种**线性表**数据结构。它用一组**连续的内存空间**，来存储一组具有**相同类型的数据**。  

数组的**优点**是可以实现**随机访问**，这也是得益于数组的定义：连续的内存空间和相同的数据类型。

尽管数组根据下标定位数据的时间复杂度是O(1)，但是在数组中的一个元素的时间复杂度不是O(1)。即使是对排好序的数组，使用二分法查找数组元素其时间复杂度也是O(logN)；而对于乱序的查找查找，仍然需要遍历数组，时间复杂度为O(N)。

数组的**缺点**是其“插入”和“删除”操作比较低效。

为了保持数组的顺序性，在插入或删除时需要挪动大量元素，挪动大量元素的过程是非常低效的。

为了缓解数组“插入”和“删除”的低效，可以采用以下的方法。

+ 使用标记删除+批量删除代替立即删除，将需要删除的数据线标记，之后统一进行批量删除（类似于JVM标记清除算法）

+ 插入元素时，直接将被插入位置的元素放到最后(O(1))，不再挪动后面元素

很多的编程语言都提供了对数组的封装，如Java的ArrayList。这些容器将数组插入、删除数据的细节进行封装，并且支持动态扩容。但这并不代表着数组就一无是处了。

+ Java中容器，如ArrayList无法直接基础数据类型，储存int、long等基础数据类型时，需要Autoboxing、Unboxing，存在性能损坏
+ 如果数据规模预先已知，并且对数据的操作简单，可以直接使用数组
+ 在定义高维数组时，数组的表现方式比容器更加直观

### 链表

链表是一种线性表数据结构。它通过**指针**将一组**零散的内存块**（结点）串联在一起用于储存数据。链表主要有三种结构：单链表、双链表和循环链表。

**单链表**

单链表有两个特殊节点，把第一个节点称为**头节点**，把最后一个节点称为**尾节点**。

得益于链表的储存特性，在对单链表进行插入或删除时，只需要修改指针，而不再需要挪动大量元素。也正是因为这个特性，使得链表无法实现“随机访问”，如果要定位一个元素，需要从头节点进行遍历扫描。

**循环链表**

循环链表与单链表唯一不同是尾节点，单链表的尾节点next域是NULL，而循环链表的尾节点next是指向链表头节点。循环链表的优点是从链表的尾节点到头节点比较方便。

**双向链表**

双链表中每个节点具有两个指针，一个指向前驱节点，一个指向后继节点。尽管双链表需要额外的两个空间来储存前驱节点和后继节点，但是给链表操作带来了灵活性。Java中的LinkedList也是使用双链表来实现。

> 双向链表的好处。在删除单链表的一个元素的时候，需要操作当前元素的前一个元素，对于单链表，只能通过从头到尾进行遍历，时间复杂度为O(N)。此时如果使用双链表，只需要同 prev 指针获取到就可以获取到其前驱节点。

**链表编码技巧**

链表节点的改动涉及到指针/引用的改动，编程难点相对数组大很多。在对链表进行编码时，可以利用下面技巧来减少出现错误的可能。

+ 警惕指针丢失和内存泄漏  
+ 利用哨兵简化实现难度
+ 重点留意边界条件处理（链表为空/只包含一个节点/只包含两个节点/处理头和尾节点）
+ 举例画图，辅助思考

### 数组与链表

数组和链表都是线性数据结构，但是采用的却是截然不同的内存组织方式，这也导致了它们具有不同的特性。

| 对比      | 数组                 | 链表               |
| --------- | -------------------- | ------------------ |
| 插入/删除 | O(N)                 | O(1)               |
| 随机访问  | O(1)                 | O(N)               |
| CPU缓存   | 连续内存，可以利用   | 离散内存，无法利用 |
| 内存要求  | 要求连续内存         | 不要求连续内存     |
| 扩容      | 需要重新创建新数组   | 天然支持动态扩容   |
| 适用场景  | 内存使用苛刻，查询多 | 删除、插入多       |

树一种非线性结构，树的家族中，主要有二叉搜索树、平衡二叉树、红黑树以及递归树。

### 栈

栈是一种操作受限（**先进后出**）的**线性表**，只允许从一端插入和删除数据。

从功能上来看，数组和链表都可以替代栈。事实上，栈底层正是通过数组或链表的方式来实现，对数组或链表的操作进行限制就可以实现一个栈。

栈是一种基础的数据结构，使用场景很多。

**函数调用栈**

操作系统会为每个线程分配一块独立的内存空间，这块内存会被组织成为栈结构，用来储存函数调用时的临时变量。每进入一个函数，都会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。

> 事实上，并不一定非得使用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。

**表达式求值**

除了运用于函数调用栈，在编译器在进行表达式求值的时候，可以通过栈来实现。

使用栈实现表达式求值时，主要流程如下。（简化为只包含加减乘除的四则运算）

+ 编译器使用两个栈分别保存**操作数**和**运算符**
+ 将运算表达式从左到右遍历
  + 遇到数字，压入操作数栈
  + 遇到运算符，先与运算符栈栈顶元素进行比较
    + 比栈顶运算符优先级高，当前运算符压栈
    + 比栈顶运算符优先级低，取出栈顶运算符，从操作数栈的栈顶取出两个操作数运算，将结果压入操作数栈，继续比较

**括号匹配**

除此之外，栈还可以用于检测表达式中的括号是否匹配。

使用栈判断括号匹配时，主要流程如下。（简化为只包含三种括号，()/[]/{}）

+ s使用一个栈来保存未匹配的左括号，从左往右扫描字符串。
+ 扫描到左括号时，将其压栈；扫描到右括号时，将其与栈顶匹配（“(”与“)”、“[”与“]”、“{”与“}”）
+ 如果不匹配则表示不匹配直接返回false；如果匹配则将其出栈，并继续
+ 如果遍历完字符串，栈顶元素已经没有元素，则表示匹配成功
+ 如果遍历到右括号返现栈中没有元素，则表示不匹配

### 队列

与栈类似，队列也是一种操作受限（**先进先出**）的**线性表**，只允许从一端插入（队尾入队）和另一端删除数据（队头出队）。

#### 实现

**队列简单实现-数组**

在使用数组实现队列时，需要使用两个指针来分别指向队头(head)和队尾(tail)。随着出队和入队的不断进行，两个指针最终可以都会往数组尾部方向移动，直到队尾到达了数组尾部。

这时，尽管数组前方仍有很多空间，但是由于队尾到达了数组尾部(`tail==arr.length-1`)，无法新插入数据。因此，当插入元素时发现已经到达了数组尾部，可以将数据整体迁移到数组前端。

![datastruct_queue_data_move](https://gitee.com/tobing/imagebed/raw/master/datastruct_queue_data_move.png)

**循环队列**

使用数组实现队列，当tail==n时，会有数据搬移操作，只会导致入到入队操作受到影响。这时，可以采用循环队列的方式来避免数据搬移。

使用数组来实现循环队列时，我们将数组逻辑地看成一个环，首尾相连。数组中用tail和head来分别记录队尾和队首。随着对循环队列的出队和入队，tail和head不断往前移动，最终都会经过下标为length-1的节点。**由于已经将数组看成一个逻辑的环，当tail和head再次往前移动的时，就会到达下标为0节点。**（这时，tail和head向前移动优雅的处理方式是tail=(tail+1)%length）

使用循环队列时，当发现(tail+1)%length=head，即表示tail**追上**了head，即循环队列的空间已经使用完毕了。（为了区分循环队列为空的状态：tail==head，因此此处浪费了一个空间来区分队列满和空）

上面都是通过数组来实现一个队列，除此之外也可以通过链表来实现。

#### 应用

作为一种基础的数据结构，队列应用广泛，如高性能队列Disruptor、Linux环形缓存，都使用了循环并发队列；JUC并发包利用ArrayBlockingQueue来实现公平锁等。

**阻塞队列与并发队列**

**阻塞队列**就是在队列基础上添加了阻塞操作。简单而言，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。  

通过上述的定义，实际上实现了一个“生产者-消费者模型”。

基于阻塞队列实现“生产者-消费者模型”可以**有效地协调生产者和消费的速度**。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。  

> Java中，使用你线程池的时候，线程池没有空闲线程时，新的任务请求线程资源时，一般有有两种处理策略。
>
> 第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。如果希望线程是能够公平地被处理，排队的线程可以通过队列来储存。
>
> 这种队列的是实现可以基于数组或链表来实现。
>
> 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。  
>
> 基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的
> 请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。  （注意：循环队列的长度设定需要对并发数据有一定的预测，否则会丢失太多请求。）

**并发队列**就是保证多个线程可以同时安全地操作队列。

最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。

## 树

树一种非线性结构，树的家族中，主要有二叉搜索树、平衡二叉树、红黑树以及递归树。

在一棵树中，一个节点可以有多个子节点，一个子节点仅能有一个父节点，同一个父节点的若干节点之间互称为**兄弟节点**。把没有父节点的节点叫做**根节点**，把没有子节点的节点叫做**叶子节点**。

在树中，有几个比较相似的概念：

+ 节点的高度：节点到叶子节点的最长路径（从下往上度量）
+ 节点深度：根节点到这个节点记录的边的个数（从上往下的度量）
+ 节点的层：节点的深度+1
+ 树的高度：根节点的高度

### 二叉树 

二叉树(Binary Tree)中，每个节点最多有两个子节点，分别是**左子节点**和**右子节点**。二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。

二叉树中，有两种比较特殊的二叉树：**满二叉树**和**完全二叉树**

+ 满二叉树：叶子节点全部都在底层，除了叶子节点之外，每个节点都有左右两个子节点
+ 完全二叉树：叶子节点在最底下两层，最后一层的叶子节点都靠走排列，并且处理最后一层，其他层的节点个数都要达到最大。

满二叉树特征明显， 因此会通过一个单独的概念来描述。相比之下，完全二叉树的特征就显得没有这么明显。之所以单独使用完全二叉树这个概念来单独描述一种二叉树，是因为这种二叉树比较特殊的特性，涉及到二叉树的储存。

#### 二叉树储存

二叉树的储存可以有两种方式，一种是基于指针或引用的**二叉链式储存法**，一种是基于数组的**顺序存储法**。

**链式存储法**简单、直观。每个节点有三个字段，其中一个储存数据，另外两个是指向左右子节点的指针。通过根节点可以通过左右子节点的指针把整棵树串起来。

```java
class Node {
    private Object e;
    private Node left, right;
}
```

**顺序存储法**基于数组实现。它把根节点储存到下标为` i=1 `的位置，左节点储存到下标为` 2*i=2 `的位置，右节点储存到下标为 `2*i+1=3 `的位置。以此类推，左节点的左子节点储存在`2*i=2*2=4` 的位置，右节点储存在`2*i+1=2*2+1=5`的位置。

在使用顺序存储法的时候，如果储存的一个树是完全二叉树，那么它**浪费**的空间仅仅是下标为0的储存位置。如果是非完全二叉树，将会浪费比较多的数组存储空间。

综上所述，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要**存储额外的左右子节点的指针**。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。  

#### 二叉树遍历

二叉树的经典遍历算法有三种：**前序遍历**，**中序遍历**和**后序遍历**。

+ 前序遍历：对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。
+ 中序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。
+ 后序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。  

实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。  

在前、中、后序遍历中，每个节点最多被访问两次，因此遍历操作的时间复杂，跟根节点个树n成正比，即二叉树遍历的时间复杂度为O(n)。

> 前序遍历，中序遍历，后序遍历的输出序列特点，[参考博客](https://www.cnblogs.com/jiaxin359/p/9512348.html)

```java
——----------------
|根节点|左子树|右子树|
------------------
——----------------
|左子树|根节点|右子树|
------------------
——----------------
|左子树|右子树|根节点|
------------------ 
```

### 二叉查找树

二叉查找树(Binary Search Tree)是二叉树中最常用的一种类型，也叫二叉搜索树,是为了实现**快速查找**而生的。但是，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。

**二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。**  

#### BST 查找操作

在二叉查找树中查找一个节点。

先取根节点，如果它等于我们要查找的数据，那就返回。

如果要查找的数据比根节点的值小，那就在左子树中递归查找；

如果要查找的数据比根节点的值大，那就在右子树中递归查找。  

#### BST 插入操作

插入过程类似于查找过程。新插入的节点一般都是在叶子节点上，在插入时只需要从根节点，依次比较要插入的数据和节点的大小关系。
如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点
的位置；如果不为空，就再递归遍历右子树，查找插入位置。

同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。  

#### BST 删除操作

<font style="color:red">**删除操作比较复杂。**</font>针对要删除节点的子节点个数不同，需要分三种情况处理。

+ 删除的节点没有子节点，直接将父节点中，指向删除节点的指针置为null即可；
+ 删除的节点的节点只有一个子节点（左/右），只需要更新父节点中，指向要删除节点的指针，让它指向删除节点的子节点即可；
+ 删除的节点的节点有两个子节点，这种情况比较复复杂。需要先找到这个节点的最小节点，把它替换到删除的节点。再删除这个最小节点，因为最小节点肯定没有左子节点（如果有就不是最小子节点了），因此，可以应用上述的两条规则来删除这个最小节点。

关于BST的输出操作，还有一个比较简单、取巧的方法，就是单纯将要删除的节点**标记**为“已删除”，但并不真正从树中将这个节点去掉。这样一来就不需要执行上述的操作，同时这种方式也没有增加插入、查找操作代码实现的难度。这种方式的唯一缺点就是比较浪费内存空间。

#### BST 其他操作

除了插入、删除、查找，BST还支持**快速地查找最大节点和最小节点、前驱节点和后继节点**。  除此之外，BST还有一个重要的特性，即

<font style="color:red">**中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。**  </font>

#### BST 支持重复数据

在实际的软件开发中，BST储存的是包含很多字段的对象。利用对象的某个字段作为键值(key)来构建二叉树。这种情况下，把对象汇总其他字段叫做**卫星数据**。

为了能够储存重复数据，可以有两种解决方法。

+ 第一种比较简单。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和
  支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。

+ 第二种方法比较不好理解，不过更加优雅。  每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。  

  这种情况下，当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。

  对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。  

#### BST 时间复杂度分析

实际上，二叉查找树的形态各式各样。它们的查找、插入、删除操作的执行效率都是不一样的。有的叉查找树，根节点的左右子树极度不平衡，退化成了链表，所以查找的时间复杂度就变成了O(n)。  对于完全二叉树、满二叉树，他们的查找、插入、删除操作也会发生相应的变化。

从上面来看，<font style="color:red">**不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。**  </font>这样一来，就**可以把求时间复杂度问题，转换为求一颗包含n个节点的完全二叉树的高度。**

#### BST 存在意义

散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)  ，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？  

主要有下面几个原因：

+ 散列表的数据时无序储存的，如果要输出有序数据，需要先排序。对于BST，只需要中序遍历可以**在O(n)时间复杂度输出有序的数据序列**。
+ 散列表扩容耗时多，且当遇到散列冲突时，性能不稳定，尽管BST不稳定，但是工程中，**最常用的平衡BST性能非常稳定，时间复杂度稳定在O(logN)**。
+ 笼统来讲，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logN 小，所以实际的查找速度可能不一定比 O(logN) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
+ **散列表的构造比二叉查找树要复杂，需要考虑的东西很多。**比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。  
+ 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然**会浪费一定的存储空间**。  

#### BST 代码实现

+ [插入](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L65)
+ [包含](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L91)
+ [前序遍历【递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L118)
+ [中序遍历【递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L140)
+ [后序遍历【递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L163)
+ [前序遍历【非递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L186)
+ [中序遍历【非递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L218)
+ [后序遍历【非递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L212)
+ [层序遍历 ](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L224)
+ [获取最大值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L260)
+ [获取最小值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L249)
+ [删除最大值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L316)
+ [删除最小值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L224)
+ [删除任意元素](https://github.com/Tobingindex/java-code/blob/master/java-datastruct/src/main/java/top/tobing/tree/binary_search_tree/BST.java#L342)



## 题目汇总

### 链表相关

单链表反转

链表中环的检测

两个有序的链表合并

删除链表倒数第 n 个结点

求链表的中间结点  

### 栈相关

leetcode上关于栈的题目大家可以先做20,155,232,844,224,682,496.  

表达式求值、括号匹配

